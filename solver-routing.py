import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import numpy as np
from datasets import load_dataset, Dataset
import json
import re

# ======= Load Model and Tokenizer =======
def load_model_tokenizer(model_path, device='auto'):
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=device,
    ).eval()  # Load and set to eval mode

    tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=32768)
    return model, tokenizer


# ======= Define System Prompts =======
system_templates = [
    "You are an mathematics expert in solving problems and question answering. You first think through the reasoning process step‐by‐step in your mind and then provide the user with the answer.",
    "You are a logician expert with advanced knowledge in reasoning, diagnostics, and treatment planning. You first think through the reasoning process step‐by‐step in your mind and then provide the user with the answer.",
    "You are an analyst specializing in logical reasoning and Yes/No/Uncertain classification. You first think through the reasoning process step‐by‐step in your mind and then provide the user with the answer.",
    # "You are skilled in logical reasoning and question answering. You first think through the reasoning process step‐by‐step in your mind and then provide the user with the answer."
]

temperatures = [
    0.1, 
    0.4, 
    0.4, 
    # 0.1
]

def select_prompt_llm(model, tokenizer, question):
    # system_message as few-shot prompt
    system_message = """\
    You are a text classifier.  Given an input text, respond with exactly one word: math, or others.  Output only that word—no extra text.

    Examples:
    Text: How many total credits does Duy have after resolving HT grades?
    Category: math
    
    Text: Should Student A be confirmed as cheating and suspended from the exam, considering the signs from the exam, behavior during the exam, AI analysis, personal history, and the possibility of appeal being denied, and assessing additional penalties if there is evidence from the camera?
    Category: others
    """

    user_message=(
        "Now classify this:\n"
        f"Text: {question}\nCategory: "
    )
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user",   "content": user_message}
    ]

    # apply chat template and tokenize
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([prompt], return_tensors="pt").to(model.device)

    # Generate at most 2 tokens
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=2,
        temperature=0.0,
        do_sample=False,
        streamer=TextStreamer(tokenizer,skip_special_tokens=True)
    )
    # only record the part generated by the model
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    # decode to text
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return response


# ======= Select Prompt Type =======
def select_prompt(question, model, tokenizer):
    q = question.lower()
    if "\nA." in q:
        return 1
    # elif any(q.startswith(w) for w in ["can", "have", "has", "do", "does", "are", "is", "should"]):
    #     return 2
    # else:
    #     return 0
    else:
        output = select_prompt_llm(model, tokenizer, question)
        output = output.split(" ")[-1]
        if (output == "others"): return 2
        elif (output == "math"): return 0
        else: return 0

# ======= Build the Input Prompt =======
def build_user_prompt(context, question, prompt_idx):
    base_instruction = "Carefully read the context and strictly follow the rules: "
    context = str(context)
    if prompt_idx == 1:  # Multiple choice
        lines = question.split("\n")
        stem = lines[0]
        choices = "\n".join(lines[1:])
        instruction = base_instruction + "select the best answer based on the provided context."
        user_prompt = (
            f"### Context ###\n{context}\n\n"
            f"{instruction}\n\n"
            f"### Question ###\n{stem}\n\n"
            f"### Choices ###\n{choices}\n\n"
            "YOU MUST SHOW your reasoning in <reasoning>…</reasoning> tags, "
            "and your final selected option letter in <answer>…</answer> tags."
        )
    else:  # Math / Classify / QA
        instruction = base_instruction + "answer the question based on the provided contexts."
        user_prompt = (
            f"### Context ###\n{context}\n\n"
            f"{instruction}\n\n"
            f"### Question ###\n{question}\n\n"
            "STRICT RULES: Show your reasoning with used contexts in <reasoning>…</reasoning> tags, "
            "and return the final answer in <answer>…</answer> tags."
        )
    return user_prompt

# ======= Generate Output =======
def generate_response(model, tokenizer, context, question, prompt_idx):
    system_prompt = system_templates[prompt_idx]
    user_prompt = build_user_prompt(context, question, prompt_idx)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
        {"role": "assistant", "content": "Let me solve this step-by-step."}
    ]

    input_text = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

    streamer = TextStreamer(tokenizer, skip_special_tokens=True)
    outputs = model.generate(
        **model_inputs,
        max_new_tokens=10000,  # shorter generation for efficiency
        temperature=temperatures[prompt_idx],
        streamer=streamer,
    )

    # Decode only new tokens
    new_tokens = [out[len(inp):] for inp, out in zip(model_inputs.input_ids, outputs)]
    response = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]
    return response

# ===== Load and Process Test Data =====
# (You can replace this with another real dev/test set if you have)
data_file = "/mlcv2/WorkingSpace/Personal/quannh/Project/Project/TRNS-AI/data/train_v1.json"

with open(data_file, "r") as f:
    data = json.load(f)

# Extract and flatten
data_dict = {
    "premises-NL": [item["premises-NL"] for item in data],
    "premises-FOL": [item["premises-FOL"] for item in data],
    "questions": [item["questions"] for item in data],
    "answers": [item["answers"] for item in data],
    "idx": [item["idx"] for item in data],
    "explanation": [item["explanation"] for item in data],
}

flattened = []
num_examples = len(data_dict["questions"])

for i in range(num_examples):
    premises = data_dict["premises-NL"][i]
    questions = data_dict["questions"][i]
    answers = data_dict["answers"][i]
    explanations = data_dict["explanation"][i]

    for q, a, e in zip(questions, answers, explanations):
        flattened.append({
            "premises-NL": premises,
            "question": q,
            "answer": a,
            "explanation": e
        })
# Build Dataset
dataset = Dataset.from_list(flattened)

# ===== LOAD MODELS =====
# math_model, math_tokenizer = load_model_tokenizer("./model/qwen-qa", device="cuda:0")
# math_model, math_tokenizer = load_model_tokenizer("Qwen/Qwen2.5-Math-7B-Instruct", device="auto")
# math_model, math_tokenizer = load_model_tokenizer("Qwen/Qwen2.5-1.5B-Instruct", device="auto")
multiple_choice_model, multiple_choice_tokenizer = load_model_tokenizer("./model/qwen-multiple-choice", device="auto")
classify_model, classify_tokenizer = load_model_tokenizer("./model/qwen-classify", device="auto")
base_model, base_tokenizer = load_model_tokenizer("Qwen/Qwen2.5-7B-Instruct", device="auto")

models = [
    base_model,
    multiple_choice_model, 
    classify_model,
]

tokenizers = [
    base_tokenizer, 
    multiple_choice_tokenizer, 
    classify_tokenizer,
]


# ===== Evaluation =====
def extract_answer(text, prompt_idx=0):
    # Match the text inside <answer>...</answer> tags
    match = re.search(r"<answer>\s*(.*?)\s*</answer>", text, re.DOTALL)
    if match:
        answer = match.group(1).strip()

        # Split the answer by whitespace (or any other delimiter)
        split_answer = answer.split()  # Default split by whitespace
        
        # Check if the first part of the split answer is "Yes" or "No"
        if "yes" in split_answer[0].lower():
            return "Yes"  # Return "Yes" or "No" if found in the first part
        elif "no" in split_answer[0].lower():
            return "No"
        else:
            return answer  # Otherwise, return the full extracted answer
    else:
        return None
    
def extract_reasoning(text):
    # Match the text inside <reasoning>...</reasoning> tags
    match = re.search(r"<reasoning>\s*(.*?)\s*</reasoning>", text, re.DOTALL)
    if match:
        # Extract the reasoning and split it into a list based on newlines
        reasoning = match.group(1).strip()
        return reasoning.strip().split('\n')  # Split by newline into a list
    else:
        match = re.search(r"(reasoning|Reasoning|Implications)[^<]*(.*?)<answer>", text, re.DOTALL)
        if match:
            # Return the reasoning found before the answer
            return match.group(2).strip()  # Group 2 contains the text between 'Reasoning' and '<answer>'
        else:
            return None
    
# ===== Testing Loop =====
def infer_dataset(dataset) -> dict:
    num_correct = 0
    num_total = 0

    for sample in dataset.select(range(100)):  # Test on 100 samples
        gold_answer = sample["answer"].strip()[0]
        question = sample["question"]
        context = sample["premises-NL"]
        prompt_idx = select_prompt(question)
    
        model = models[prompt_idx]
        tokenizer = tokenizers[prompt_idx]
        response = generate_response(
            model=model,
            tokenizer=tokenizer,
            context=context,
            question=question,
            prompt_idx=prompt_idx,
        )

        pred_answer = extract_answer(response)
        print(response)

        print(f"\nQuestion idx {num_total + 1}:")
        print(f"Question:{question}\n")
        print(f"Gold: {gold_answer} | Predicted: {pred_answer}")

        if pred_answer == gold_answer:
            num_correct += 1
        num_total += 1

    # ===== Compute and Print Accuracy =====
    accuracy = num_correct / num_total * 100
    print(f"\nEvaluation completed on {num_total} samples.")
    print(f"Accuracy: {accuracy:.2f}%")
    
import json
import os
from datetime import datetime

# ===== Testing Loop =====
def infer_one_sample(context: list, questions: list):
    num_correct = 0
    num_total = 0
    results = {}
    answers = []
    reasons = []

    for question in questions:
        prompt_idx = select_prompt(question, base_model, base_tokenizer)
        print(f"Prompt idx: {prompt_idx}")
        input()
        model = models[prompt_idx]
        tokenizer = tokenizers[prompt_idx]
        response = generate_response(
            model=model,
            tokenizer=tokenizer,
            context=context,
            question=question,
            prompt_idx=prompt_idx,
        )

        pred_answer = extract_answer(response)
        pred_reason = extract_reasoning(response)
        answers.append(pred_answer)
        reasons.append(pred_reason)
        
        print(response)

        print(f"\nQuestion idx {num_total + 1}:")
        print(f"Question:{question}\n")
        print(f"Predicted: {pred_answer}")
    
    results["answers"] = answers
    results["explanation"] = reasons
    
    # Get the current time in day-hour-minute-second format
    current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    
    # Define the filename based on the current time
    filename = f"./output/results_{current_time}.json"
    
    # Save results to the file
    with open(filename, 'w') as f:
        json.dump(results, f, indent=4)

    print(f"Results saved to {filename}")
    return results

    

if __name__ == "__main__":
    # premises = [
    #     "Total program credits are 120; internship eligibility requires ≥ 78 credits (65%), GPA ≥ 6.0 (scale 0–10), and ≥ 50 internship prep hours.",
    #     "First-year students (Year 1) have < 40 credits.",
    #     "Failed courses (grade < 4.0) contribute 0 credits.",
    #     "Improvement retakes (grade ≥ 4.0) use highest grade, no extra credits.",
    #     "Each course (grade ≥ 4.0) adds prep hours: 3 credits = 5 hours, 4 credits = 7 hours, 5 credits = 10 hours.",
    #     "Third-year students (Year 3) with < 78 credits can join prep workshops (20 hours), if GPA ≥ 5.5.",
    #     "A student (Year 2) has a GPA of 6.2, 80 credits, 120 prep hours, including C1 (3 credits, 5.8, 5 hours), C2 (4 credits, 6.5, 7 hours).",
    #     "The student took C3 (5 credits, 7.0), retook C1 (6.8), took C4 (3 credits, 3.5, failed)."
    # ]
    
    # questions = [
    #     "What is the student’s updated GPA after all course attempts?",
    #     "How many internship prep hours has the student accumulated, and are they eligible for an internship?"
    # ]
    premises = [
            "If someone completes the safety training, then they are allowed to operate heavy machinery.",
            "If someone is allowed to operate heavy machinery, then they must have signed the safety compliance form.",
            "All employees receive regular safety reminders.",
            "If someone takes the advanced training course, then they receive safety reminders.",
            "If someone has a supervisor's recommendation, then they have completed the safety training.",
            "If someone is allowed to operate heavy machinery, then they have taken the advanced training course.",
            "If someone has signed the safety compliance form, then they have taken the advanced training course.",
            "All employees have completed the safety training.",
            "If someone has signed the safety compliance form, then they received a supervisor’s recommendation.",
            "If someone does not receive safety reminders, then they did not receive a supervisor’s recommendation.",
            "If someone receives safety reminders, then they have completed the safety training.",
            "If being allowed to operate heavy machinery implies taking the advanced training course, then all employees receive regular safety reminders.",
            "If there exists someone with a supervisor’s recommendation, then there exists someone who receives safety reminders.",
            "If someone has signed the safety compliance form, then if someone has a supervisor’s recommendation, someone receives safety reminders.",
            "If taking the advanced training means receiving safety reminders, then not receiving reminders means not having a supervisor’s recommendation.",
            "If being allowed to operate machinery means signing the safety compliance, then if being allowed to operate machinery means taking the advanced training course, then all employees receive safety reminders.",
            "If someone has signed the safety compliance form, then they have completed the safety training.",
            "If someone has not signed the safety compliance form, then they do not have a supervisor’s recommendation.",
            "All employees are allowed to operate heavy machinery.",
            "If someone does not have a supervisor’s recommendation, then they are not allowed to operate heavy machinery.",
            "If someone has not completed the safety training, then they have not taken the advanced training course."
    ]
    
    questions = [
            "Have all employees signed the safety compliance form?",
            "Have all employees taken the advanced training course?"
        ]
    infer_one_sample(context=premises, questions=questions)
    
    # answers =  [
    #     "6.28",
    #     "130, Yes"
    # ]
    
    # infer_dataset(dataset)
